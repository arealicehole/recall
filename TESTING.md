# ğŸ§ª Testing Guide for Recall Application

This guide covers the comprehensive test suite for the Recall audio transcription application, including desktop GUI tests, web API tests, and unit tests.

## ğŸ—‚ï¸ Test Structure

```
recall/
â”œâ”€â”€ tests/                      # Unit tests directory
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_config.py         # Config class tests
â”‚   â”œâ”€â”€ test_audio_handler.py  # Audio processing tests
â”‚   â”œâ”€â”€ test_transcriber.py    # Transcription tests
â”‚   â””â”€â”€ test_web_api.py        # Web API tests
â”œâ”€â”€ test_gui.py                # Desktop GUI tests
â”œâ”€â”€ run_tests.py               # Main test runner
â”œâ”€â”€ pytest.ini                # pytest configuration
â”œâ”€â”€ debug_transcription.py     # Integration tests
â”œâ”€â”€ check_audio_content.py     # Audio content tests
â””â”€â”€ scripts/test_deployment.sh # Web deployment tests
```

## ğŸš€ Quick Start

### Run All Tests
```bash
python run_tests.py
```

### Run Specific Test Categories
```bash
# Unit tests only
python run_tests.py --unit

# GUI tests only
python run_tests.py --gui

# Integration tests only
python run_tests.py --integration

# Web deployment tests only
python run_tests.py --web
```

### Run Tests with Verbose Output
```bash
python run_tests.py --verbose
```

## ğŸ“¦ Test Dependencies

Install testing dependencies:
```bash
pip install -r requirements.txt
```

### Required Testing Packages
- **pytest**: Unit testing framework
- **pytest-mock**: Mocking utilities
- **pytest-cov**: Code coverage reporting
- **customtkinter**: GUI framework (for GUI tests)
- **flask**: Web framework (for web API tests)

## ğŸ§ª Test Categories

### 1. Unit Tests (`tests/` directory)

#### Config Tests (`test_config.py`)
- âœ… Configuration loading
- âœ… Environment variable handling
- âœ… API key management
- âœ… Output directory validation
- âœ… Supported file formats
- âœ… Default values
- âœ… Configuration updates

#### Audio Handler Tests (`test_audio_handler.py`)
- âœ… Audio file processing
- âœ… Format validation
- âœ… File existence checks
- âœ… AMR format handling
- âœ… Temporary file cleanup
- âœ… Error handling
- âœ… Path handling
- âœ… Unicode filename support
- âœ… Concurrent access
- âœ… Memory management

#### Transcriber Tests (`test_transcriber.py`)
- âœ… Transcription initialization
- âœ… API key configuration
- âœ… Successful transcription
- âœ… Error handling
- âœ… Empty result handling
- âœ… Speaker identification
- âœ… Network error handling
- âœ… Rate limit handling
- âœ… Timeout handling
- âœ… Large file processing
- âœ… Status handling

#### Web API Tests (`test_web_api.py`)
- âœ… Flask app initialization
- âœ… Route testing
- âœ… File upload handling
- âœ… API endpoints
- âœ… Error responses
- âœ… File format validation
- âœ… Security headers
- âœ… Rate limiting
- âœ… Memory management
- âœ… Concurrent requests

### 2. GUI Tests (`test_gui.py`)

#### Desktop Application Tests
- âœ… Application initialization
- âœ… Window creation
- âœ… Menu system
- âœ… File selection components
- âœ… Progress tracking
- âœ… Output directory selection
- âœ… Configuration loading
- âœ… Dialog functionality
- âœ… Error handling
- âœ… Threading safety
- âœ… Transcription workflow

### 3. Integration Tests

#### Core Functionality (`debug_transcription.py`)
- âœ… End-to-end transcription
- âœ… API connectivity
- âœ… Audio processing pipeline
- âœ… Error diagnosis
- âœ… Configuration validation

#### Audio Content Analysis (`check_audio_content.py`)
- âœ… Audio content detection
- âœ… Volume analysis
- âœ… Speech detection
- âœ… File format support
- âœ… Quality assessment

### 4. Web Deployment Tests (`scripts/test_deployment.sh`)

#### Production Deployment
- âœ… Docker container health
- âœ… API endpoint testing
- âœ… SSL certificate generation
- âœ… nginx configuration
- âœ… Production deployment
- âœ… Load balancing
- âœ… Security validation
- âœ… Kubernetes deployment

## ğŸ“Š Test Coverage

### Current Coverage Areas

| Component | Unit Tests | Integration Tests | GUI Tests | Web Tests |
|-----------|------------|------------------|-----------|-----------|
| **Config** | âœ… | âœ… | âœ… | âœ… |
| **Audio Handler** | âœ… | âœ… | âœ… | âœ… |
| **Transcriber** | âœ… | âœ… | âœ… | âœ… |
| **GUI Application** | âŒ | âœ… | âœ… | âŒ |
| **Web API** | âœ… | âœ… | âŒ | âœ… |
| **Deployment** | âŒ | âœ… | âŒ | âœ… |

### Coverage Goals
- **Unit Tests**: 80%+ code coverage
- **Integration Tests**: Full workflow coverage
- **GUI Tests**: Complete UI interaction coverage
- **Web Tests**: All endpoints and deployment scenarios

## ğŸ”§ Running Tests

### Method 1: Main Test Runner (Recommended)

```bash
# Run all tests with comprehensive reporting
python run_tests.py

# Run specific test categories
python run_tests.py --unit --verbose
python run_tests.py --gui
python run_tests.py --integration
python run_tests.py --web

# Skip dependency check
python run_tests.py --skip-deps
```

### Method 2: Direct pytest

```bash
# Run unit tests only
pytest tests/

# Run specific test file
pytest tests/test_config.py

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run with verbose output
pytest tests/ -v
```

### Method 3: Individual Test Scripts

```bash
# GUI tests
python test_gui.py

# Integration tests
python debug_transcription.py
python check_audio_content.py

# Web deployment tests
bash scripts/test_deployment.sh
```

## ğŸ“‹ Test Configuration

### pytest Configuration (`pytest.ini`)
```ini
[tool:pytest]
testpaths = tests
addopts = --strict-markers --tb=short --cov=src --cov-report=term-missing
markers =
    slow: marks tests as slow
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    gui: marks tests as GUI tests
    web: marks tests as web API tests
    network: marks tests that require network access
```

### Environment Variables for Testing
```bash
# Optional: Set test API key
export ASSEMBLYAI_API_KEY=test_key_here

# Optional: Set test output directory
export OUTPUT_DIRECTORY=/tmp/test_output

# Optional: Enable test mode
export TESTING=true
```

## ğŸ¯ Test Execution Examples

### Complete Test Suite
```bash
# Run everything
python run_tests.py

# Expected output:
# ğŸš€ Recall Application Test Suite
# ğŸ” Checking test dependencies...
# ğŸ§ª RUNNING UNIT TESTS
# ğŸ–¥ï¸  RUNNING GUI TESTS
# ğŸ”— RUNNING INTEGRATION TESTS
# ğŸŒ RUNNING WEB DEPLOYMENT TESTS
# ğŸ“Š TEST REPORT
# ğŸ‰ All tests completed successfully!
```

### Unit Tests Only
```bash
# Fast unit tests
python run_tests.py --unit --verbose

# Or directly with pytest
pytest tests/ -v
```

### GUI Tests Only
```bash
# Desktop GUI tests
python run_tests.py --gui

# Or directly
python test_gui.py
```

### Integration Tests Only
```bash
# Core functionality tests
python run_tests.py --integration

# Or run individually
python debug_transcription.py
python check_audio_content.py
```

### Web Deployment Tests Only
```bash
# Production deployment validation
python run_tests.py --web

# Or directly (requires Docker)
bash scripts/test_deployment.sh
```

## ğŸ› Troubleshooting

### Common Issues

#### 1. Missing Dependencies
```bash
# Install all dependencies
pip install -r requirements.txt

# Or skip dependency check
python run_tests.py --skip-deps
```

#### 2. GUI Tests Fail on Headless Systems
```bash
# Set display variable (Linux)
export DISPLAY=:0

# Or use virtual display
xvfb-run python test_gui.py
```

#### 3. Web Tests Fail Without Docker
```bash
# Install Docker first
# Then run web tests
python run_tests.py --web
```

#### 4. Permission Issues
```bash
# Make scripts executable
chmod +x scripts/test_deployment.sh
```

### Debug Mode
```bash
# Run tests with maximum verbosity
python run_tests.py --verbose

# Run specific test with debug info
pytest tests/test_config.py -v -s
```

## ğŸ“ˆ Continuous Integration

### GitHub Actions Example
```yaml
name: Test Suite
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python run_tests.py
```

### Local Pre-commit Hook
```bash
# Add to .git/hooks/pre-commit
#!/bin/bash
python run_tests.py --unit
```

## ğŸ” Writing New Tests

### Adding Unit Tests
1. Create test file in `tests/` directory
2. Follow naming convention: `test_*.py`
3. Use pytest fixtures and assertions
4. Add appropriate markers

### Adding GUI Tests
1. Add test methods to `test_gui.py`
2. Use mocking for external dependencies
3. Test user interactions and workflows

### Adding Integration Tests
1. Create new script or add to existing ones
2. Test complete workflows
3. Include in `run_tests.py`

## ğŸ“š Best Practices

### Test Organization
- **Unit tests**: Test individual components in isolation
- **Integration tests**: Test component interactions
- **GUI tests**: Test user interface interactions
- **Web tests**: Test API endpoints and deployment

### Test Naming
- Use descriptive test names
- Follow pattern: `test_<what_is_being_tested>`
- Include expected behavior in name

### Mocking
- Mock external dependencies (APIs, file system)
- Use `pytest-mock` for clean mocking
- Mock at appropriate abstraction level

### Assertions
- Use specific assertions
- Include helpful error messages
- Test both success and failure cases

## ğŸ† Test Quality Metrics

### Success Criteria
- **All tests pass**: 100% test suite success
- **Code coverage**: >80% line coverage
- **Performance**: Tests complete in <5 minutes
- **Reliability**: Tests pass consistently

### Monitoring
- Track test execution time
- Monitor code coverage trends
- Identify flaky tests
- Regular test maintenance

---

## ğŸ“ Support

For testing-related questions:
1. Check existing test files for examples
2. Review this documentation
3. Run tests with `--verbose` for detailed output
4. Use `pytest --help` for pytest options

Happy testing! ğŸ‰ 